# 一、模型组件
卷积，池化， 1x1卷积， BN/LN，全连接层，dropout，残差连接（输入输出维度相同），embedding、注意力模块、激活函数等
## 1、 卷积
    计算公式(图像尺寸-卷积核尺寸 + 2*填充值)/步长+1
    cove1d：用于文本数据，只对宽度进行卷积，对高度不进行卷积
    cove2d：用于图像数据，对宽度和高度都进行卷积
    1. 卷积核（convolutional kernel）：可以看作对某个局部的加权求和；它是对应局部感知，它的原理是在观察某个物体时我们既不能观察每个像素也不能一次观察整体，而是先从局部开始认识，这就对应了卷积。卷积核的大小一般有1x1,3x3和5x5的尺寸（一般是奇数x奇数）。
    2. 卷积核的个数就对应输出的通道数（channels），这里需要说明的是对于输入的每个通道，输出每个通道上的卷积核是不一样的。比如输入是28x28x192(WxDxK,K代表通道数)，然后在3x3的卷积核，卷积通道数为128，那么卷积的参数有3x3x192x128，其中前两个对应的每个卷积里面的参数，后两个对应的是卷积核总的个数（一般理解为，卷积核的权值共享只在每个单独通道上有效，至于通道与通道间的对应的卷积核是独立不共享的，所以这里是192x128）
## 2、池化
    计算公式(图像尺寸-池化窗尺寸 + 2*填充值)/步长+1
    作用：特征融合和降维，减少参数和计算量，减轻模型过拟合
    maxpooling有局部不变性而且可以提取显著特征的同时降低模型的参数，从而降低模型的过拟合。
    卷积特征往往对应某个局部的特征。要得到global的特征需要将全局的特征执行一个aggregation（聚合）。池化就是这样一个操作，对于每个卷积通道，将更大尺寸（甚至是global）上的卷积特征进行pooling就可以得到更有全局性的特征。这里的pooling当然就对应了cross region（即区域交叉注意信息交互）。与1x1的卷积相对应，而1x1卷积可以看作一个cross channel（即通道交叉注意信息交互）的pooling操作。
## 3、1x1卷积
    当输入为6x6x32时，1x1卷积的形式是1x1x32，当只有一个1x1卷积核的时候，此时输出为6x6x1。此时便可以体会到1x1卷积的实质作用：降维。
    
    1、1x1卷积一般只改变输出通道数（channels），而不改变输出的宽度和高度
    2、降维和升维
    3、1*1卷积核，可以在保持feature map尺度不变的（即不损失分辨率）的前提下大幅增加非线性特性（利用后接的非线性激活函数），把网络做的很deep。
    4、跨通道信息交互：使用1x1卷积核，实现降维和升维的操作其实就是channels 间信息的线性组合变化，3x3，64channels的卷积核后面添加一个1x1，28channels的卷积核，就变成了3x3，28channels的卷积核，原来的64个channels就可以理解为跨通道线性组合变成了28channels，这就是通道间的信息交互
## 多通道卷积

## BN和LN的实现细节
    BN减少内部协变量偏移ICS加快模型训练。ICS问题是，
    一、每个神经元的输入数据不再是“独立同分布”。
    二、上层参数需要不断适应新的输入数据分布，降低学习速度。
    三、下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。
    四、每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。

    一、BatchNorm是在样本维度进行归一化，在一个批次内不同样本的相同特征计算均值和方差，适合结构化数据比如C,H,W的图片
    以图片（B,C,H,W）为例，在不同样本相同通道（包括宽高，也就是（B,H,W）间进行均值方差计算，B*H*W个数计算均值和方差），但是其可训练向量（标准差和偏置）为torch.size（[C]）
    二、而LayerNorm是在特征维度进行归一化，适合非结构化数据，比如文本数据，因为每个文本序列长度不同，在同一个样本的不同特征之间计算均值和方差。
    比如B,S,N的文本数据，在n_embedding维度即N个数做均值和方差计算，其可训练参数是torch.size（[N]）
## 激活函数
    答：激活函数在神经网络中起着非常重要的作用，它们的存在有以下几个关键原因：
    一、引入非线性、增加网络的表达能力： 激活函数引入了非线性性质，允许神经网络模型学习复杂的非线性映射关系。如果没有激活函数，多层神经网络就会等效于单一线性变换，无法拟合复杂的数据模式。合适的激活函数可以增加神经网络的表达能力，使其能够表示各种复杂的函数。这有助于网络学习到更多不同类型的特征和抽象概念。
    二、非线性特征提取： 激活函数可以将输入数据中的非线性特征提取出来，从而使神经网络能够更好地处理实际问题，如图像、文本和语音识别等。
    三、解决梯度消失问题： 合适的激活函数可以帮助缓解梯度消失问题，使梯度能够在反向传播中更好地传播。例如，ReLU（Rectified Linear Unit）和其变种具有较大的梯度，有助于减轻梯度消失问题。
    三、稀疏性和稳定性： 一些激活函数，如Sigmoid和Tanh，可以将输入数据映射到有界区间（双侧饱和），防止数值爆炸，稳定网络
    四、非线性分类和回归： 在分类和回归任务中，激活函数可以将网络输出映射到适当的范围内，使其具有语义上的意义，例如将输出解释为类别概率或连续值。
### 常用激活函数

    Sigmoid：双侧饱和，0均值，导数最大为0.25
![本地路径](conclusion\sigmoid.png)

    tanh：双侧饱和，0均值，导数0处最大，为1
![本地路径](conclusion\tanh.png)
    relu ， leaky relu， erelu，为了解决梯度消失和梯度爆炸问题
    优点：relu解决了梯度消失、爆炸的问题
            计算方便，计算速度快
            加速了网络的训练
    缺点：由于负数部分恒为0，导致一些神经元无法激活(可通过设置小学习率部分解决)
            输出并不是(零均值)零中心化的
## embedding

## 注意力模块



# 二、 分布式训练
    DistributedDataParallel支持模型并行，而DataParallel并不支持，这意味如果模型太大单卡显存不足时只能使用前者；
    DataParallel是单进程多线程的，只用于单机情况，而DistributedDataParallel是多进程的，适用于单机和多机情况，真正实现分布式训练；
    DistributedDataParallel的训练更高效，因为每个进程都是独立的Python解释器，避免GIL问题，而且通信成本低其训练速度更快；
    DistributedDataParallel中每个进程都有独立的优化器，执行自己的更新过程，但是梯度通过通信传递到每个进程，所有执行的内容是相同的。

以数据并行随机梯度下降(SGD)技术为例，神经网络训练的过程如下:

1，首先需要通过在第一个step进行Broadcast操作将参数同步到集群内的所有的训练卡上;
2，将数据样本切片分发到整个集群的每张训练卡上并且通过data Loader技术将数据样本加载进训练卡的高速内存空间内，作为输入X;
3，每个训练卡在其数据样本上运行前向传播，计算出误差LOSSi；
4，对计算出的LOSSi进行反向传播，得到梯度GRADi；
5，所有的训练卡在主机内及主机之间进行集合通信并进行梯度归约(AllReduce)；
6，最后再进行参数更新以获得新的梯度参数。

本质上分布式训练是数据加载、前向传播、反向传播、集合通信（RllReduce）以及参数更新这5个步骤的逻辑组合，
从上面的步骤可知分布式训练是在固定的步骤迭代中进行的，并且需要系统内的所有的训练卡都完成它们的迭代步骤，才能进行最后的参数更新，因此会有'木桶效应'。这相当于在单个训练卡上执行梯度下降技术，但是通过在系统内所有的训练卡之间分发数据样本并同时执行计算来获得训练的加速。

## Ring AllReduce
第一种常见的实现算法是基于Ring的AllReduce，英伟达的NCCL通信库采用了这种算法，该算法每次跟相邻的两个节点进行通信，每次通信数据总量的N分之一。
该算法的优点是实现简单，能充分利用每个节点的上行和下行带宽；缺点是通信延迟随着节点数线性增加，为2*（N-1），特别是对于小包延迟增加比较明显
## Having Double AllReduce
另一种常见的AllReduce实现算法是Having-Doubling，阿里巴巴的通信库ACCL采用了这种算法，该算法每次选择节点距离倍增的节点相互通信，每次通信量倍减（或倍增）
该算法的优点是通信步骤较少，只有logN次（其中N表示参与通信的节点数）通信即可完成，所以其有更低的延迟。缺点是每一个步骤相互通信的节点均不相同，通信链接来回切换会带来额外开销。


# 三、 transformer模型相关参数量，计算量，中间激活，内存需求
记transformer的层数为$l$, 隐藏层维度$h$, 注意力头数$a$。词表大小$V$, 批次为$b$, 序列长度$s$
## 模型参数量
self.attention 块的模型参数有$Q$,$K$,$V$的权重矩阵和偏置，输出权重矩阵$W_O$和偏置。四个权重矩阵形状为$[h,h]$,4个偏置形状为$[h]$。自注意力块参数量为 $4h^2 + 4h$

MLP由2个线性层组成，一般情况下，第一个线性层将h映射到4h,第二个再将4h映射到h。第一个线性层权重$W_1$和偏置分别为$[h,4h]$, $[4h]$;第二个线性层权重$W_2$和偏置为$[4h,h]$, $[h]$.MLP块参数量为$8h^2+5h$

自注意力块和MLP块各有一个LN，包含两个可训练参数：缩放参数和平移参数，形状是隐藏层维度$[h]$,参数总量为$4h$

每个transformer层参数为$12h^2 + 13h$

除此以外，词嵌入矩阵的参数量也较多，词向量维度通常等于隐藏层维度 $h$，词嵌入矩阵的参数量为 
$Vh$。最后的输出层的权重矩阵通常与词嵌入矩阵是参数共享的。
此外，如果采用可训练式的位置编码，会有一些可训练模型参数，数量比较少。如果采用相对位置编码，例如RoPE和ALiBi，则不包含可训练的模型参数。我们忽略这部分参数。

综上， l层transformer模型的可训练模型参数量为 $l(12h^2 + 13h) + Vh$,当$h$较大时，忽略一次项，模型参数量近似为$12lh^2$。

估计不同版本LLaMA模型参数量

### 训练过程中的显存占用分析

在训练神经网络的过程中，占用显存的大头主要分为四部分：模型参数、前向计算过程中产生的中间激活、后向传递计算得到的梯度、优化器状态。这里着重分析参数、梯度和优化器状态的显存占用，中间激活的显存占用后面会详细介绍。训练大模型时通常会采用AdamW优化器，并用混合精度训练来加速训练，基于这个前提分析显存占用。

在一次训练迭代中，每个可训练模型参数都会对应1个梯度，并对应2个优化器状态（Adam优化器梯度的一阶动量和二阶动量）。设模型参数量为$\Phi$，那么梯度的元素数量为 $\Phi$，AdamW优化器的元素数量为 $2\Phi$。float16数据类型的元素占2个bytes，float32数据类型的元素占4个bytes。在混合精度训练中，会使用float16的模型参数进行前向传递和后向传递，计算得到float16的梯度；在优化器更新模型参数时，会使用float32的优化器状态、float32的梯度、float32的模型参数来更新模型参数。因此对每个可训练参数，占用$(2+4)+(2+4)+(4+4)$一共20字节,float16和float32的模型参数，float16和float32的梯度和float32的两个优化器状态。
使用AdamW优化器和混合精度训练来训练参数量为$\Phi$的大模型，模型参数、梯度和优化器状态占用的显存大小为 $20\Phi$字节。

### 推理过程显存占用
在神经网络的推理阶段，没有优化器状态和梯度，也不需要保存中间激活。少了梯度、优化器状态、中间激活，模型推理阶段占用的显存要远小于训练阶段。模型推理阶段，占用显存的大头主要是模型参数，如果使用float16来进行推理，推理阶段模型参数占用的显存大概是$2\Phi$字节。如果使用KV cache来加速推理过程，KV cache也需要占用显存。此外，输入数据也需要放到GPU上，还有一些中间结果（推理过程中的中间结果用完会尽快释放掉，不过这部分占用的显存是很小的，可以忽略。

## 计算量FLOPs估计






# Transformer
## 位置编码
Why: 对于任何一门语言，单词在句子中的位置以及排列顺序是非常重要的，它们不仅是一个句子的语法结构的组成部分，更是表达语义的重要概念。一个单词在句子的位置或排列顺序不同，可能整个句子的意思就发生了偏差。
Transformer模型抛弃了RNN、CNN作为序列学习的基本模型。我们知道，循环神经网络本身就是一种顺序结构，天生就包含了词在序列中的位置信息。当抛弃循环神经网络结构，完全采用Attention取而代之，这些词序信息就会丢失，模型就没有办法知道每个词在句子中的相对和绝对的位置信息。因此，有必要把词序信号加到词向量上帮助模型学习这些信息，位置编码（Positional Encoding）就是用来解决这种问题的方法
HOW: 
它能为每个时间步输出一个独一无二的编码；
不同长度的句子之间，任何两个时间步之间的距离应该保持一致；
模型应该能毫不费力地泛化到更长的句子，它的值应该是有界的；
它必须是确定性的；
引入序列词元之间相对信息，即对于固定偏差k，PE(t+k)只与k有关，不与t相关。

## 注意力机制和卷积/RNN
卷积是自注意力机制的特例，自注意力是卷积的一般化形式。
 
On the Relationship between Self-Attention and Convolutional Layers
More generally, fully-attentional models seem to learn a generalization of CNNs where the kernel pattern is learned at the same time as the filters—similar to deformable convolutions.
更一般地说，完全注意力模型似乎学习了CNN的一般化（注意全局），其中内核模式与过滤器同时学习（自己去学习如何过滤哪些信息以及重点关注哪些范围的信息）——类似于可变形卷积。
RNN的特点在于，对于一个输入序列的某个词元，它只考虑该词元之前输入的词元信息（这里指最一般的单向RNN，双向RNN也是可以考虑双向信息的），且如果是一个很长的序列，那么开头的词元信息就很难保留到末尾的词元；
自注意力可以获取到全局的信息，而且对于一个词元，它和任意位置的词元的关联计算都是一样的，不会有长序列信息丢失的问题。
在运算方面，由于RNN的结构限制，每一个词元输出的计算是不能并行执行的，必须要先等之前输入的词元完成计算；自注意力，我们上面讲过了，可以很好地表示成矩阵运算的形式，使得各个词元输出的计算可以同时进行。
## 注意力机制
1、自注意力
2、多头自注意力
3、通道注意力机制（SENet）
4、空间注意力机制
空间注意力机制和通道注意力机制具有异曲同工之妙，通道注意力机制旨在捕捉通道的重要性的程度，空间注意力机制旨在通过引入注意力模块，使模型能够自适应地学习不同区域的注意力权重。这样，模型可以更加关注重要的图像区域，而忽略不重要的区域。其中，最为典型的是 CBAM（Convolutional Block Attention Module），CBAM 是一种结合了通道注意力和空间注意力的模型，旨在增强卷积神经网络对图像的关注能力。


# 五、NLP

## 六大任务
分类：从文字序列到标签的映射，如文本分类。
匹配：文字序列与文字序列的匹配，如搜索、阅读理解。
标注和语义分析：文字序列到标签序列或结构表示的映射，如分词、词性标注、句法分析。
序列生成：文字序列的生成，也就是基于语言模型的生成。
序列到序列（seq2seq）：文字序列到文字序列的转化，如机器翻译、生成式对话、摘要。
序贯决策：基于已有的文字序列产生新的文字序列，如多轮对话。

前三个是语言理解任务，后三个是语言生成任务。理解任务的输出是类别标签等，可以认为是心智语言的表示。
所有的任务都可以用序列到序列 seq2seq 模型实现。语言理解是自然语言到心智语言的 seq2seq。语言生成是心智语言到自然语言的 seq2seq。语言转换是一种自然语言到另一种自然语言的转换。

## 文本分类

文本分类问题： 给定文档p（可能含有标题t），将文档分类为n个类别中的一个或多个
文本分类应用： 常见的有垃圾邮件识别，情感分析
文本分类方向： 主要有二分类，多分类，多标签分类
文本分类方法： 传统机器学习方法（贝叶斯，svm等），深度学习方法（fastText，TextCNN等）

文本分类的处理大致分为文本预处理、文本特征提取、分类模型构建等。和英文文本处理分类相比，中文文本的预处理是关键技术。

### 文本预处理（解决特征空间高维性、语义相关性和特征分布稀疏）
1、中文分词技术
为什么分词处理？因为研究表明特征粒度为词粒度远远好于字粒度，其大部分分类算法不考虑词序信息，基于字粒度的损失了过多的n-gram信息。

中文分词主要分为两类方法：基于词典的中文分词和基于统计的中文分词。

基于词典的中文分词
核心是首先建立统一的词典表，当需要对一个句子进行分词时，首先将句子拆分成多个部分，将每一个部分与字典一一对应，如果该词语在词典中，分词成功，否则继续拆分匹配直到成功。所以字典，切分规则和匹配顺序是核心。
基于统计的中文分词方法
统计学认为分词是一个概率最大化问题，即拆分句子.基于语料库，统计相邻的字组成的词语出现的概率，相邻的词出现的次数多，就出现的概率大，按照概率值进行分词，所以一个完整的语料库很重要。
基于理解的分词方法
基于理解的分词方法是通过让计算机模拟人对句子的理解，达到识别词的效果。其基本思想就是在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象。它通常包括三个部分：分词子系统、句法语义子系统、总控部分。在总控部分的协调下，分词子系统可以获得有关词、句子等的句法和语义信息来对分词歧义进行判断，即它模拟了人对句子的理解过程。这种分词方法需要使用大量的语言知识和信息。由于汉语语言知识的笼统、复杂性，难以将各种语言信息组织成机器可直接读取的形式，因此目前基于理解的分词系统还处在试验阶段。
2、去除停用词
建立停用词字典，停用词主要包括一些副词、形容词及其一些连接词。通过维护一个停用词表，实际上是一个特征提取的过程，本质上是特征选择的一部分。
### 文本特征提取

1、词袋模型

思想：建立一个词典库，该词典库包含训练语料库的所有词语，每个词语对应一个唯一识别的编号，利用one-hot文本表示。
文档的词向量维度与单词向量的维度相同，每个位置的值是对应位置词语在文档中出现的次数，即词袋模型（BOW））

问题：
（1）容易引起维度灾难问题，语料库太大，字典的大小为每个词的维度，高维度导致计算困难，每个文档包含的词语数少于词典的总词语数，导致文档稀疏。
（2）仅仅考虑词语出现的次数，没有考虑句子词语之间的顺序信息，即语义信息未考虑
2、TF-IDF文本特征提取

利用TF和IDF两个参数来表示词语在文本中的重要程度。
TF是词频：
指的是一个词语在一个文档中出现的频率，一般情况下，每一个文档中出现的词语的次数越多词语的重要性更大，例如BOW模型一样用出现次数来表示特征值，即出现文档中的词语次数越多，其权重就越大，问题就是在长文档中的词语次数普遍比短文档中的次数多，导致特征值偏向差异情况。
TF体现的是词语在文档内部的重要性。

IDF是体现词语在文档间的重要性
即如果某个词语出现在极少数的文档中，说明该词语对于文档的区别性强，对应的特征值高，IDF值高，IDFi=log（|D|/Ni），D指的是文档总数，Ni指的是出现词语i的文档个数，很明显Ni越小，IDF的值越大。
最终TF-IDF的特征值的表达式为：TF-IDF（i，j）=TF*IDF

3、基于词向量的特征提取模型

想基于大量的文本语料库，通过类似神经网络模型训练，将每个词语映射成一个固定维度的向量，维度在几十到几百维之间，每个向量就代表着这个词语，词语的语义和语法相似性通过向量之间的相似度来判断。

常用的word2vec主要是CBOW和skip-gram两种模型，由于这两个模型实际上就是一个三层的深度神经网络，其实NNLM的升级，去掉了隐藏层，由输入层、投影层、输出层三层构成，简化了模型和提升了模型的训练速度，其在时间效率上、语法语义表达上效果明显都变好。word2vec通过训练大量的语料最终用定维度的向量来表示每个词语，词语之间语义和语法相似度都可以通过向量的相似度来表示。
### 分类模型
1、传统机器学习方法：
​ 传统机器学习算法中能用来分类的模型都可以用，常见的有：NB模型，随机森林模型（RF），SVM分类模型，KNN分类模型模型。

2、深度学习文本分类模型

fastText模型
原理： 句子中所有的词向量进行平均（某种意义上可以理解为只有一个avg pooling的特殊CNN），然后直接连接一个 softmax 层进行分类。
TextCNN
利用CNN来提取句子中类似 n-gram 的关键信息 
改进： fastText 中的网络结果是完全没有考虑词序信息的，而TextCNN提取句子中类似 n-gram 的关键信息。
TextRNN
模型： Bi-directional RNN（实际使用的是双向LSTM）从某种意义上可以理解为可以捕获变长且双向的的 “n-gram” 信息。
改进： TextCNN有个最大问题是固定 filter_size 的视野，一方面无法建模更长的序列信息，另一方面 filter_size 的超参调节也很繁琐。
TextRNN + Attention
改进：注意力（Attention）机制是自然语言处理领域一个常用的建模长时间记忆机制，能够很直观的给出每个词对结果的贡献，基本成了Seq2Seq模型的标配了。实际上文本分类从某种意义上也可以理解为一种特殊的Seq2Seq，所以考虑把Attention机制引入近来。
TextRCNN（TextRNN + CNN）

## 情感分析
一、概述
​ 情感分析是自然语言处理中常见的场景，比如淘宝商品评价，饿了么外卖评价等，对于指导产品更新迭代具有关键性作用。通过情感分析，可以挖掘产品在各个维度的优劣，从而明确如何改进产品。比如对外卖评价，可以分析菜品口味、送达时间、送餐态度、菜品丰富度等多个维度的用户情感指数，从而从各个维度上改进外卖服务。
情感分析可以采用基于情感词典的传统方法，也可以采用基于机器学习的方法。

### 基于情感词典的情感分类方法

​1、基于情感词典的方法，先对文本进行分词和停用词处理等预处理，再利用构建好的情感词典，对文本进行字符串匹配，从而挖掘正面和负面信息。
​2、情感词典包含正面词语词典、负面词语词典、否定词语词典、程度副词词典等四部分。一般词典包含两部分，词语和权重。
3、情感词典在整个情感分析中至关重要，所幸现在有很多开源的情感词典，如BosonNLP情感词典，它是基于微博、新闻、论坛等数据来源构建的情感词典，以及知网情感词典等。当然也可以通过语料来自己训练情感词典。
4、情感词典文本匹配：基于词典的文本匹配算法相对简单。逐个遍历分词后的语句中的词语，如果词语命中词典，则进行相应权重的处理。正面词权重为加法，负面词权重为减法，否定词权重取相反数，程度副词权重则和它修饰的词语权重相乘
5、利用最终输出的权重值，就可以区分是正面、负面还是中性情感了。
​基于词典的情感分类，简单易行，而且通用性也能够得到保障。但仍然有很多不足：
1、精度不高,语言是一个高度复杂的东西，采用简单的线性叠加显然会造成很大的精度损失。词语权重同样不是一成不变的，而且也难以做到准确。
2、新词发现,对于新的情感词，比如给力，牛逼等等，词典不一定能够覆盖。
3、词典构建难,基于词典的情感分类，核心在于情感词典。而情感词典的构建需要有较强的背景知识，需要对语言有较深刻的理解，在分析外语方面会有很大限制
### 基于机器学习的情感分类方法
即为分类问题，文本分类中的各方法均可采用。

## LLM 实现所有自然语言处理任务
图 3 描述基于 LLM 的语言理解，语言生成，语言转换（翻译）的 LLM。比如，思维链（chain of thought）就可以认为是心智语言的内容。基于 LLM 的语言理解就是把自然语言转化为心智语言。注意：心智语言应该是没有歧义的，而用 LLM 生成的内容，包括思维链，经常是有歧义的。

所以，可以认为 LLM 用于语言理解时生成的内容是心智语言的近似。自然语言表示心智语言的好处是人们可以很容易定义和标注数据，如思维链数据，但是缺点是不能保证不产生歧义.




# 四、联邦学习
随着大数据的发展，我们不再关注数据量的问题，而是开始关心数据的隐私和安全问题。联邦学习提供了一种隐私保护机制，可以有效地利用终端设备的计算资源对模型进行训练，防止隐私信息在数据传输过程中被泄露

## 挑战
    1、隐私保护：必须保证联邦学习中的模型训练不会泄露用户的隐私信息。
    2、数据量不足：在传统机器学习中，如果想要得到一个较好的模型，往往需要大量的数据，但在分布式环境中，每个移动设备上的数据量不足。另一方面，以集中的方式收集所有的数据可能导致巨大的费用。因此，联邦学习要求每个设备使用本地数据来训练本地模型（数据量不足），然后将所有本地模型上传到服务器上聚合成全局模型。
    3、数据异质性：联邦环境中存在大量边缘设备，这些设备所持有的数据可能是非独立同分布的
    4、设备异质性：边缘设备的计算能力，存储能力，通信条件等各有不同，影响模型参数交换和局部迭代。
## 分类
数据分区、隐私机制、适用的机器学习模型、通信体系结构和解决异质问题的方法五个方面总结了联邦学习的分类
### 数据划分
    根据数据的样本空间和特征空间的不同分布模式，联邦学习可以分为三大类：横向联邦学习、纵向联邦学习和联邦迁移学习。
    1、横向联邦学习适用于两个数据集的用户特征重叠较多而用户重叠较少的情况。比如在两个不同的地区，两个当地电力局提供的服务类似，因此对用户产生了类似的数据，比如月用电量，因此两个电力局的数据集特征重叠较多。但是，这两个地区的用户分别受各自电力局的管辖，两个电力局的数据集用户重叠较少
    2、纵向联邦学习适用于两个数据集用户特征重叠较少，但用户重叠较多的情况。例如同一个地区有两个机构：银行和电力局。银行产生用户的收入和支出行为等数据，电力局产生用户用电数据，两个数据集的特征几乎没有重叠，但两个数据集中的用户却几乎是同一批用户（有较大重叠）
    3、联邦迁移学习适用于两个数据集的用户特征和用户都重叠较少的情况。 在这种情况下，我们不分割数据，但可以使用迁移学习克服数据或标签的缺乏。比如地区1的银行和地区2的电力局，用户重叠少（2个地区），用户特征重叠也较少（银行数据和电力数据）。
    迁移学习最适合的情况是当你试图优化任务的性能，但没有足够的相关数据用于训练。例如，医院放射科很难收集大量的X线扫描片来建立良好的放射诊断系统。此时，迁移学习将帮助我们学习其他相关但不同的任务，如图像识别任务，学习一个放射诊断系统。通过联邦迁移学习，既可以保证数据的私密性，又可以将辅助任务的模型迁移到指导者学习中，解决了数据量小的问题。

### 隐私保护机制
保护联邦隐私的常用方法是模型聚合、同态加密和差分隐私。

### 可应用的机器学习模型
我们主要考虑联邦学习支持的三种模型：线性模型、决策树和神经网络。
### 通信体系结构
### 解决异质性的办法
在联邦学习的应用场景中，设备的差异会使整个训练过程的效率低下。为了解决系统异质问题，有四种方式：异步通信、设备采样、容错机制和模型异质。异步通信即异步联邦学习；设备采用即设备选择。
主要看一下模型异质性即数据异质性。为了解决统计数据异质的问题，联邦学习网络主要分为三种建模方法：单个设备有自己的模型，个性化联邦学习；培训适用于所有设备的全局模型；为任务训练相关的学习模式。

## 应用
    谷歌键盘、智能医疗诊断、无线通信方面
## 挑战
    1、隐私代价：虽然目前有一些提高数据私密性的方法，但这些方法都增加了计算复杂度。为了进一步有效地保护私有数据的安全，我们需要寻找新的方法来防止私有数据在模型传输过程中被泄露。
    2、通信成本：数百万个远程移动设备，这意味着联邦学习模型的训练可能涉及大量的通信。
    3、系统异质性：每个设备的计算和通信能力可能不同，由于硬件和网络连接的不同，联邦网络中每个设备的计算和通信能力可能不同，不可靠，离线等
    4、不可靠模型上传：在联邦学习中，移动节点可能会有意或无意地误导服务器聚合全局模型。对于故意的行为，攻击者可以发送恶意的模型参数来影响全局模型的聚合，从而造成模型训练的错误。另一方面，不稳定的移动网络环境可能会导致移动设备出现一些意想不到的行为，比如上传一些低质量的模型，这些都会对联邦学习产生不利影响。因此，抵抗这种不可靠的本地模型上传至关重要。
## 方向
    1、隐私限制：详细级别的批处理设备的隐私限制，基于特定设备隐私限制的隐私保护方法
    2、通信成本和计算成本的权衡：提高通信效率主要可以考虑两个方面：迭代发送小消息，或者减少沟通的总轮数。寻找通信成本与计算压力之间的平衡是未来工作的主要方向。
    3、分层分簇联邦学习：最近的一些研究表明，如果能够提前获得系统中设备的异质性，则可以根据异质性对所有移动设备进行分组，并为每组分配一个本地中心服务器。我们可以首先聚合一组类似的异质设备模型，然后将它们发送到服务器以聚合成一个全局模型。
    4、设备选择：如何在联邦学习任务中找到值得信赖和可靠的客户端是至关重要的。一些论文中引入了声誉的概念，作为衡量客户可靠性的指标。与区块链结合。
## 总结
一方面，当数据不足导致用户无法训练出满意的模型时，联邦学习可以聚合多方用户模型，在不暴露原始数据的情况下对集成的模型进行更新。另一方面，当用户没有足够的数据标签学习时，联邦学习不仅可以为用户提供安全的模型共享机制，还可以将模型迁移到特定的任务中，解决数据标签不足的问题。



# 六、 强化学习
大致分为两类，基于值函数的策略迭代强化学习算法，基于策略梯度的策略迭代强化学习算法。
基于值函数的强化学习通过递归地求解贝尔曼方程来维护Q值函数（可以是离散的列表，也可以是神经网络），每次选择动作时会选择该状态下对应Q值最大的动作，使得未来积累的期望奖励值最大。经典的基于值函数的强化学习算法有Q-Learning、SARSA、DQN算法等。这些算法在学习后的Q值函数不再发生变化，每次做出的策略也是一定的，可以理解为确定性策略。基于策略的强化学习不再通过价值函数来确定选择动作的策略，而是直接学习策略本身，通过一组参数$\theta$对策略进行参数化，并通过BP神经网络方法优化策略网络$\theta$

主要区别：基于策略的强化学习用参数化概率分布$\pi_\theta(a|s)=P(a|s,\theta)$代替了基于值函数的强化学习中的确定性策略$\phi:s-> a$
，在返回的动作概率列表中对不同的动作进行抽样选择。
基于策略梯度理论如下
目标函数 ${{\max }_{\theta }}J(\theta )={{\max }_{\theta }}{{E}_{\tau \sim{{\pi }_{\theta }}}}R(\tau )={{\max }_{\theta }}\sum\limits_{\tau }{P}(\tau ;\theta )R(\tau )$
最大化所有轨迹（奖励*概率）之和，将轨迹进行拆分，轨迹在$\pi_\theta(a|s)$下由状态转移概率和动作选择概率组成，其中状态转移概率只和环境有关。
因此经过合理计算得到$J(\theta)$对$\theta$的导数为
![本地路径](conclusion\1.png)
采样轨迹样本或者一段轨迹可以对策略进行梯度更新
只需定义损失函数如下：$\mathcal{L}(a,s,r)=-\log \left( {{\pi }_{\theta }}(a\mid s) \right)r$
## REINFORCE 算法

## 自然策略梯度算法
在传统的策略梯度算法中，我们根据目标函数梯度和步长 来更新策略权重 ，这样的更新过程可能会出现两个常见的问题：

过冲（Overshooting）：更新错过了奖励峰值并落入了次优策略区域
下冲（Undershooting）：在梯度方向上采取过小的更新步长会导致收敛缓慢
在监督学习问题中，overshooting并不是什么大问题，因为数据是固定的，我们可以在下一个epoch中重新纠正，但在强化学习问题中，如果因为overshooting陷入了一个较差的策略区域，则未来的样本批次可能不会提供太多有意义的信息，用较差的数据样本再去更新策略，从而陷入了糟糕的正反馈中无法恢复。较小的学习率可能会解决这个问题，但会导致收敛速度变慢的undershooting问题。
为了避免overshooting带来的严重后果，一种直觉方法是限制每次更新步长的上限：
$\delta\theta^*=argmaxJ(\theta+\delta\theta), where ||\delta\theta||<=\epsilon$, 
尽管听起来很合理，但实际效果并不是像我们预期的那样，原因是不同的分布对参数变化的敏感度是不同的。比如在下图中，我们都让策略权重变化了1个单位的欧式距离，但是对左图的影响是远大于右图的。所以，只限制参数是不合理的，更应该考虑分布对参数变化的敏感度，传统的策略梯度算法无法考虑到这种曲率变化，我们需要引入二阶导数，这正是自然策略梯度相较于传统策略梯度算法的区别。
### 限制策略更新的差异
我们需要表示策略（分布）之间的差异，而不是参数本身的差异。计算两个概率分布之间的差异，最常见的是KL散度，也称为相对熵，描述了两个概率分布之间的距离：

通过计算这个表达式，我们可以确保在参数空间中执行大更新的同时，保证策略本身的改变不超过阈值。然而，计算KL散度需要遍历所有的状态-动作对，因此我们需要一些化简来处理现实的RL问题。以下略

最终结果在两个方面不同于传统的策略梯度算法：
考虑到策略对局部变化的敏感性，策略梯度由逆Fisher矩阵校正，而传统的梯度方法假定更新为欧几里得距离。
更新步长 具有适应梯度和局部敏感性的动态表达式，确保无论参数化如何，策略变化幅度为$\epsilon$。在传统方法中，$\alpha$通常设置为一些标准值，如0.1或0.01。
## TRPO
Trust region policy optimization（TPRO）算法是现代强化学习的基础，它以自然策略梯度优化为基础，迅速获得普及，成为主流强化学习算法，因为它在经验上比自然策略梯度算法表现得更好、更稳定。尽管此后它已被近端策略优化 (PPO) 超越，但它的仍然具有重要的意义。

我们将讨论TRPO背后的单调改进定理（关注直觉）以及将其与自然策略梯度区分开的三个变化。
3.1 自然策略梯度算法的缺陷
1、近似值可能会违反KL约束，从而导致分析得出的步长过大，超出限制要求
2、Fisher矩阵的逆矩阵 的计算时间太长，是O(N3)复杂度的运算
3、我们没有检查更新是否真的改进了策略。由于存在大量的近似过程，策略可能并没有优化
针对自然策略梯度算法的问题，我们希望可以对策略的优化进行量化，从而保证每次的更新一定是优化作用的。为此，我们需要计算两种策略之间预期回报的差异。这里采用的是原策略预期回报添加新策略预期优势的方式。该表达式在原策略下计算优势函数，无需重新采样：

### 算法实现
在实际的算法实现方面，TRPO和自然策略梯度算法没有太大的区别。TRPO主要有三个改进，每个改进都解决了原始算法中的一个问题。TRPO的核心是利用单调改进定理，验证更新是否真正改进了我们的策略。
1、共轭梯度法
2、线搜索：虽然自然梯度策略中提供了给定KL散度约束的最佳步长，但由于存在较多的近似值，实际上可能不满足该约束。TRPO 通过执行线搜索来解决此问题，通过不断地迭代减小更新的大小，直到第一个不违反约束的更新。这个过程可以看作是不断缩小信任区域，即我们相信更新可以实际改进目标的区域。
3、改进检查：在TRPO中，我们并没有假设更新会提高替代优势，而是真正检查了它。尽管实际计算时需要根据旧策略计算优势，以及使用重要性抽样来调整概率，会花费一些时间，但验证更新是否真正改进了策略是有必要的。

## PPO

TRPO算法解决了许多与自然策略梯度相关的问题，并获得了RL社区的广泛采用。但是，TRPO仍然存在一些缺点，特别是：

无法处理大参数矩阵：尽管使用了共轭梯度法，TRPO仍然难以处理大的 Fisher矩阵，即使它们不需要求逆
二阶优化很慢：TRPO的实际实现是基于约束的，需要计算上述Fisher矩阵，这大大减慢了更新过程。此外，我们不能利用一阶随机梯度优化器，例如ADAM
TRPO 很复杂：TRPO很难解释、实现和调试。当训练没有产生预期的结果时，确定如何提高性能可能会很麻烦。
### PPO Penalty
PPO通过设置目标散度 的方式解决了这个问题，希望我们的每次更新都位于目标散度附近的某个地方。目标散度应该大到足以显著改变策略，但又应该小到足以使更新稳定。
每次更新后，PPO都会检查更新的大小。如果最终更新的散度超过目标散度的 1.5倍，则下一次迭代我们将加倍 $\beta$来更加重惩罚。相反，如果更新太小，我们将$\beta$ 减半，从而有效地扩大信任区域。迭代更新的思路与TRPO线搜索有一些相似之处，但PPO搜索是在两个方向上都有效的，而TRPO是单向减小的。
与自然策略梯度和TRPO算法相比，PPO更容易实现。同时，我们可以使用流行的随机梯度下降算法（如ADAM等）执行更新，并在更新太大或太小时调整惩罚。总之，PPO比TRPO更容易使用，同时不失竞争力。
### PPO Clip

## 策略梯度算法总结
从传统策略梯度算法，到自然策略梯度算法，再到TRPO算法，以及最终的PPO算法，经过不断的优化迭代，PPO算法已经成为强化学习领域最主流的算法。不论是学术界中的各大顶会文章，还是工业界中例如chatgpt的背后强化学习部分的实现，都离不开PPO算法的身影。
纵向来看，对策略梯度算法的改进，主要针对的就是限制参数迭代的这一步。自然策略梯度算法引入了KL散度约束，TRPO利用线搜索和改进检查来保证限制下的可行性，PPO则通过clip函数限制了策略可以改变的范围等。
相比于自然梯度和TRPO所具有的理论保证和数学技巧，PPO放弃了一些数学上的严谨性，但往往能比其竞争对手更快更好地收敛。看来PPO在速度、严谨性和可用性之间取得了正确的平衡，在未来几年来依旧会保有属于它的竞争力。




# 六、实习

## 应用双向注意力机制，提升模型性能 10%

BiDAF的思想，Passenge和Question进行双向注意力分数计算，
1、passenge和question进行单词嵌入，单词维度相同，为embed_dim。
2、计算匹配矩阵M，相似度函数用点积。
 

3、分别计算context2question注意力与question2context的注意力
Context-to-query Attention(C2Q)计算的是对每一个 context word 而言，哪些 query words 和它最相关；对相似度矩阵取softmax得到注意力，然后与query编码矩阵U相乘得到新的query U^。
query-to-context Attention(Q2C)计算的是对每一个 query word 而言，哪些 context words 和它最相关；取相似度矩阵M值最大的那一列做softmax，然后重复T次得到注意力，与context编码矩阵H相乘得到新的context H^ 
4、最终向量表达为正文、问题双向注意力融合的拼接矩阵。有多种拼接方式。

## 抓取上游数据并处理分析，对数据清洗筛选，构建训练用数据集，提出数据优化方案。
感知融合后的诸如自车信息，障碍物信息，车道线信息和导航信息，主要包括速度、加速度、相对位移、类型等，类型变量编码为one-hot离散向量，之前只包括egomotion比如巡航、直行、左转右转、左变道右变道，车道线障碍物等类型编码为一位token（0-9等），不合理，已改进为one-hot向量，因为是类型，因此我们希望模型预测到相应类，和分类任务相似。

## 引入障碍物 TTC 和相关数据类型one-hot编码表示。
## 构建预测可视化模块，按条件筛选预测数据集，预测 BEV 图、指标统计并利用 Gradio 进行网页可视化部署。
1、主要难点在于数据的筛选，初始方案包括时间戳、车号、ego_motion、数量，由于时间戳不好挑选，主要由车号和ego_motion进行筛选，实际主要是挑选的导航信息，因为数量有限并且容易输入。要求预测token具有足够的挑选ego_motion时间帧，而且同一个场景很重要。
2、用于放入模型预测的token和用于计算指标和画图的token对齐非常重要。
3、判断同一个场景主要是看时间戳连续与否，我怎么判断的，其他人怎么判断，别人写了什么bug
4、Bev图是根据每一帧的感知融合信息，利用cv2模块进行自车、障碍物、车道线画图，主要用到cv2.rectangle()、cv2.polylines()；需要车辆（矩形）与图片像素的比例尺，自车和障碍物大小之间比例尺以及相对距离与整图片的比例尺；同时加上车号，时间戳，相关指标信息等，用到cv2.putText函数；后期加上预测点和真实轨迹点，用到cv2.circle。
5、Gradio本地服务器端口可视化。

## 参与云端认知大模型 DriveGPT 研发，进行基于 GPT-2 的模型训练和测试，基于 RLHF 机制实现对话级的决策模型。
实习业务目标：参与自动驾驶云端认知大模型研发，利用logsim（公司仿真软件）评测云端模型，提供拟人化轨迹，打分高于车端模型20%, 评测指标主要为ade,fde, L2error，collision rate ）

## 应用交叉注意力机制，参与多模态输入，包括文本和 BEV 图片，基于 BLIP-2 的方法进行多模态预训练，设计多模态轨迹输出。
1、交叉注意力与上述双向注意力类似，可以看成上下文信息和目标点的交叉注意。
2、Bev图根据文本信息利用openCV库进行绘制，利用原始BLIP-2的模型架构即Q-former，只进行query的学习，文本序列模型为自己模型，图像编码模型为BLIP2模型ViT。
3、多模态输出表示输出多个航路点。






# 八、BLIP-2
## 背景
BLIP提出了一种基于预训练的方法，通过联合训练视觉和语言模型来提升多模态任务的性能。
BLIP-2则提出了一种更简洁的预训练方法，利用现有的单模态视觉和文本预训练模型，以减少计算成本和避免灾难性遗忘问题。
## 解决问题
解决问题如下：
问题一：现有的预训练模型仅在基于理解或生成的任务中表现出色。
问题二，同时从web收集的图文对含有大量噪声，训练是次优的。
## 数据层面
针对数据，训练数据来自网络图文对，包含大量噪声，所以增加了一个在线数据打标签和清理的任务，把处理好的数据继续用来迭代原模型。BLIP通过引导字幕有效地利用了嘈杂的web数据，其中字幕器（captioner）生成合成字幕，而过滤器（ﬁlter）则删除了嘈杂的字幕。
## 多任务层面
针对单独基于理解或生成的单个能力，为了实现统一的视觉语言理解和生成，BLIP引入了编码器-解码器的多模态混合结构MED（ Multimodal mixture of Encoder-Decoder），能够有效地进行多任务预学习和迁移学习。MED可以作为单模态编码器、基于图像的文本编码器或基于图像的文本解码器工作。该模型与三个视觉语言目标联合预训练：图像文本对比学习、图像文本匹配和图像条件语言建模。
## MED模型架构
MED包括两个单模态编码器（lmage Encoder，Text Encoder），一个以图像为基础的文本编码器（image-grounded text encoder）和一个以图像为基础的文本解码器（image-grounded text decoder）。具体如下：
(1) 单模态编码器，分别对图像和文本进行编码。文本编码器与BERT相同，其中将 [CLS] token附加到文本输入的开头以概括句子。
(2) Image-grounded文本编码器，通过在文本编码器的每个transformer块的自注意 (SA) 层和前馈网络 (FFN) 之间插入一个额外的交叉注意 (CA) 层来注入视觉信息。文本中附加了一个特定于任务的[Encode] token，[Encode]的输出嵌入被用作图像-文本对的多模态表示。
(3) Image-grounded文本解码器，将基于图像的文本编码器中的双向自注意力层替换为因果自注意力层。[Decode] token用于表示序列的开始，而[EOS] token用于表示其结束。

## 可改进地方
(1)多轮数据集的bootstrapping；（2） 为每幅图像生成多个合成字幕，进一步扩大预训练语料库；（3） 通过训练多个不同的字幕器和过滤器，并在CapFilt中组合他们的力量，来模拟模型集成。











# MTR

## Encoder
1、轨迹和道路图编码为折线图。对于感兴趣对象，以其中心坐标系规范化输入折线图，折现编码器对输入折线进行编码送入encoder模块。智能体Ain ∈ R^Na ×t×Ca
地图特征$M_{in} ∈ R^{N_m ×n×C_m}$。通过PointNet-比如 polyline encoder进行编码表示
$A_p =\Phi(MLP(A_{in}))$,$M_p =\Phi(MLP(M_{in}))$,$\Phi$是最大池化层用于总结智能体输入和地图输入的特征。
2、关注场景上下文的局部特征
第J层编码器层注意力模块表示为 
$${{G}^{j}}=\operatorname{MultiHeadAttn}\left( \text{ query }={{G}^{j-1}}+\text{P}{{\text{E}}_{{{G}^{j-1}}}},\operatorname{key}=\kappa \left( {{G}^{j-1}} \right)+\text{P}{{\text{E}}_{\kappa \left( {{G}^{j-1}} \right)}},\text{ value }=\kappa \left( {{G}^{j-1}} \right) \right)$$
编码器初始输入$G_0 = [Ap, Mp] ∈ R^{(Na +Nm)}×D$,$k(.)$表示K近邻算法，用于对每个query折现找到k个最近的折线，即关注最近的局部特征。
编码器输出智能体特征$A_{past} ∈ R^{Na ×D}$和地图特征$M ∈ R^{Nm ×D}$。这作为以下解码器网络的场景上下文输入。
3、考虑到智能体特征学习到丰富的智能体上下文信息，采用简单的回归头将A输出每个智能体未来的速度和位置信息
$$S_{1:T} = MLP(A_{past})$$
T是预测的时间帧，$S_i ∈ R^{Na ×4}$.这个未来轨迹同样会输入到折线编码器编码未来轨迹特征即$A_{future} ∈ R^{Na ×D}$.
最终编码信息将过去和未来信息进行融合，即$A = MLP([A_{past}, A_{future}])$
这个辅助任务为解码器提供了未来轨迹信息，利于更好的预测。消融实验证明了这个简单并且轻量级的任务能有效提高多模态轨迹预测性能。

## Decoder with Motion Query Pair
1、提出运动查询对。对于全局意图定位和局部运动细化，每个运动查询对包含静态意图查询和动态搜索查询分布负责这两个任务
2、静态意图查询。We propose static intention query to narrow down the uncertainty of future
trajectory by utilizing different intention queries for different motion modes。基于真实轨迹点利用K-means聚类算法生成K个代表性意图点$I ∈ R^{K×2}$,每个意图点代表一种运动模式，包含方向和速度。我们将每个静态意图查询建模为意图点的可学习位置嵌入，$Q_I = MLP (PE(I))$
Notably, each intention query
takes charge of predicting trajectories for a spe-
cific motion mode, which stabilizes the training
process and facilitates predicting multimodal tra-
jectories since each motion mode has their own
learnable embedding.
3、动态搜索查询。 Each dynamic searching query is
also the position embedding of a spatial point, which is initialized with its corresponding intention
point but will be dynamically updated according to the predicted trajectory in each decoder layer。
具体地，给定第j层解码器的预测的未来轨迹$Y_{1:T}^j = {Y_i^j ∈ R^{K×2} | i = 1, · · · , T }$,那么第j+1层解码器的动态搜索查询更新为
$$Q_S^{j+1} = MLP(PE(Y_T^j )$$
4、 提出了一个动态地图收集模块，通过查询与轨迹对齐的局部区域中的地图特征来提取细粒度的轨迹特征，该模块通过收集中心最接近预测轨迹的L条多段线地图来实现。由于代理的行为在很大程度上取决于路线图，这种局部运动细化策略能够持续关注最新的局部上下文信息进行迭代运动细化
5、在每个解码器层中，静态意图查询用于在不同的运动意图之间传播信息，而动态搜索查询用于从场景上下文特征中聚合轨迹特定特征。具体来说，我们使用静态意图查询作为自我注意模块的位置嵌入，如下所示：
$$C_{\text{sa}}^{j}=\text{ MultiHeadAttn }\left( \text{ query }={{C}^{j-1}}+{{Q}_{I}},\text{ key }={{C}^{j-1}}+{{Q}_{I}},\text{ value }={{C}^{j-1}} \right)$$
$C_j-1$为上一层query 内容，最开始的层初始化为0.$C_{sa}^j ∈ R^{K×D}$ 是更新的query内容
6、我们将查询和密钥的内容特征和位置嵌入连接起来，以解耦它们对注意力权重的贡献
$$\begin{align}
  & C_{A}^{j}=\operatorname{MultiHeadAttn}\left( \text{ query }=\left[ C_{\text{sa}}^{j},Q_{S}^{j} \right],\operatorname{key}=\left[ A,\text{P}{{\text{E}}_{A}} \right],\text{ value }=A \right), \\ 
 & C_{M}^{j}=\operatorname{MultiHeadAttn}\left( \text{ query }=\left[ C_{\text{sa}}^{j},Q_{S}^{j} \right],\operatorname{key}=\left[ \alpha (M),{{\operatorname{PE}}_{\alpha (M)}} \right],\text{ value }=\alpha (M) \right), \\ 
 & {{C}^{j}}=\operatorname{MLP}\left( \left[ C_{A}^{j},C_{M}^{j} \right] \right) \\ 
\end{align}$$
α（M）是前面提到的动态地图收集模块，用于收集L个轨迹对齐的地图特征，用于运动细化。
最后，$C_j∈R^{K×D}$是第j层中每个运动查询对的更新查询内容特征